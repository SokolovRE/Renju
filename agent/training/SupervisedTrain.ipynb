{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qAKQYtzbewFJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tMgCj-w1ewFX"
   },
   "outputs": [],
   "source": [
    "games_num = 1984694\n",
    "\n",
    "move_dict = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, \n",
    "             'g':6, 'h':7, 'j':8, 'k':9, 'l':10, 'm':11, \n",
    "             'n':12, 'o':13, 'p':14}\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', \n",
    "           'g', 'h', 'j', 'k', 'l', 'm', \n",
    "           'n', 'o', 'p']\n",
    "for i in range(15):\n",
    "    move_dict[i] = letters[i]\n",
    "\n",
    "def AllGraph():\n",
    "    def Conv2D(filters, kernel=5, name=None):\n",
    "        return tf.layers.Conv2D(filters, kernel, padding='same', name=name)\n",
    "\n",
    "    def BatchNorm(name=None):\n",
    "        return tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    p_dict = {}\n",
    "    r_dict = {}\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 2, 15, 15])\n",
    "    y_ = tf.placeholder(tf.int64)\n",
    "    phase = tf.placeholder(tf.bool)\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    p_dict['x'] = x\n",
    "    p_dict['y_'] = y_\n",
    "    p_dict['phase'] = phase\n",
    "    p_dict['alpha'] = alpha\n",
    "    r_dict['x'] = x\n",
    "    r_dict['y_'] = y_\n",
    "    r_dict['phase'] = phase\n",
    "    r_dict['alpha'] = alpha\n",
    "        \n",
    "    tran = tf.transpose(x, [0, 2, 3, 1])\n",
    "        \n",
    "    model = Conv2D(32)(tran)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(64)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(64)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    out = tf.layers.Flatten()(Conv2D(1, 3)(model))\n",
    "        \n",
    "    move = tf.nn.softmax(out)\n",
    "    p_dict['move'] = move\n",
    "\n",
    "    check_prediction = tf.equal(tf.argmax(out,1), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(check_prediction, tf.float32))\n",
    "    p_dict['accuracy'] = accuracy\n",
    "\n",
    "    loss_function = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=out))\n",
    "    p_dict['loss'] = loss_function\n",
    "\n",
    "    extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_ops):\n",
    "        train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n",
    "    p_dict['train_step'] = train_step\n",
    "            \n",
    "            \n",
    "    model = Conv2D(32)(tran)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(32)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    model = Conv2D(64)(model)\n",
    "    model = BatchNorm()(model, training=phase)\n",
    "    model = tf.nn.relu(model)\n",
    "        \n",
    "    out = tf.layers.Flatten()(Conv2D(1, 3)(model))\n",
    "        \n",
    "    move = tf.nn.softmax(out)\n",
    "    r_dict['move'] = move\n",
    "\n",
    "    check_prediction = tf.equal(tf.argmax(out,1), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(check_prediction, tf.float32))\n",
    "    r_dict['accuracy'] = accuracy\n",
    "\n",
    "    loss_function = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=out))\n",
    "    r_dict['loss'] = loss_function\n",
    "\n",
    "    extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_ops):\n",
    "        train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n",
    "    r_dict['train_step'] = train_step\n",
    "        \n",
    "    graph = tf.get_default_graph()\n",
    "    p_dict['graph'] = graph\n",
    "    r_dict['graph'] = graph\n",
    "            \n",
    "    return {'policy': p_dict, 'rollout': r_dict, 'graph': graph}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gTTGgo0JewFf"
   },
   "outputs": [],
   "source": [
    "def TrainGen(batch_size, left=0, right=9999999999, aug=False, end=False):\n",
    "    batch_X = np.zeros((0, 2, 15, 15))\n",
    "    batch_Y = np.zeros((0,))\n",
    "    global_epoch = 0\n",
    "    time_begin = time.time()\n",
    "    while True:\n",
    "        if global_epoch > 0 and end:\n",
    "            break\n",
    "        train_file = open('train_data.renju')\n",
    "        game_ind = 0\n",
    "        for game in train_file:\n",
    "            if game_ind < left:\n",
    "                game_ind += 1\n",
    "                continue\n",
    "            if game_ind == right:\n",
    "                break\n",
    "            game_ind += 1\n",
    "            win = game[0]\n",
    "            if win != 'b' and win != 'w':\n",
    "                continue\n",
    "            game = game.split(' ')[1:]\n",
    "            board = np.zeros((len(game), 2, 15, 15))\n",
    "            moves = np.zeros((len(game),))\n",
    "            if aug and game_ind%2 == 1:\n",
    "                aug_type = np.random.randint(0, 3)\n",
    "            player = 0\n",
    "            for ind, turn in enumerate(game):\n",
    "                x = move_dict[turn[0]]\n",
    "                y = int(turn[1:])-1\n",
    "                if aug and game_ind%2 == 1:\n",
    "                    if aug_type == 0:\n",
    "                        x = 14-x\n",
    "                        y = 14-y\n",
    "                    elif aug_type == 1:\n",
    "                        x = 14-x\n",
    "                    elif aug_type == 2:\n",
    "                        temp = x\n",
    "                        x = y\n",
    "                        y = temp\n",
    "                if ind != len(game)-1:\n",
    "                    board[ind+1] = board[ind]\n",
    "                    board[ind+1][player][x][y] = 1\n",
    "                moves[ind] = 15*x + y\n",
    "                if player == 0:\n",
    "                    player = 1\n",
    "                else:\n",
    "                    player = 0\n",
    "            batch_X = np.append(batch_X, board, axis=0)\n",
    "            batch_Y = np.append(batch_Y, moves, axis=0)\n",
    "            if len(batch_X) >= batch_size:\n",
    "                yield batch_X, batch_Y, time.time()-time_begin, global_epoch, game_ind\n",
    "                time_begin = time.time()\n",
    "                batch_X = np.zeros((0, 2, 15, 15))\n",
    "                batch_Y = np.zeros((0,))\n",
    "        global_epoch += 1\n",
    "    if len(batch_X) > 0:\n",
    "        yield batch_X, batch_Y, time.time()-time_begin, global_epoch-1, game_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_mPr5JKAewFr"
   },
   "outputs": [],
   "source": [
    "def SupervisedTrain(sess, graph_dict, epochs=5, batches_per_epoch=200, \n",
    "                    train_games=games_num-1000, \n",
    "                    validation_games=1000, batch_size=100, \n",
    "                    alph=0.0001, name='model'):\n",
    "    train_games = games_num-validation_games\n",
    "    \n",
    "    x = graph_dict['policy']['x']\n",
    "    y_ = graph_dict['policy']['y_']\n",
    "    phase = graph_dict['policy']['phase']\n",
    "    alpha = graph_dict['policy']['alpha']\n",
    "    \n",
    "    accuracy_p = graph_dict['policy']['accuracy']\n",
    "    loss_p = graph_dict['policy']['loss']\n",
    "    train_step_p = graph_dict['policy']['train_step']\n",
    "    \n",
    "    accuracy_r = graph_dict['rollout']['accuracy']\n",
    "    loss_r = graph_dict['rollout']['loss']\n",
    "    train_step_r = graph_dict['rollout']['train_step']\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, name, write_meta_graph=False)\n",
    "    \n",
    "    log_count = 0\n",
    "    last_val_acc_p = 0\n",
    "    last_val_loss_p = float('inf')\n",
    "    last_val_acc_r = 0\n",
    "    last_val_loss_r = float('inf')\n",
    "    train_gen = TrainGen(batch_size, 0, train_games)\n",
    "    begin_time = time.time()\n",
    "    train_acc_p = 0\n",
    "    train_acc_r = 0\n",
    "    \n",
    "    def info_log(log_count):\n",
    "        if log_count == 70:\n",
    "            clear_output()\n",
    "            log_count = 0\n",
    "        else:\n",
    "            log_count += 1\n",
    "            return log_count\n",
    "        log_count += 1\n",
    "        print('----------------------------------------------------')\n",
    "        print('Global epoch:', g_epoch)\n",
    "        print('Local epoch:', epoch+1, '/', epochs)\n",
    "        print('Batch:', batch_ind, '/', batches_per_epoch)\n",
    "        print('Epoch time:', (time.time()-epoch_time)/60, '/', (((time.time()-epoch_time)/batch_ind)*batches_per_epoch)/60)\n",
    "        print('Time for making one batch:', batch_time)\n",
    "        print('Game:', game_ind, '/', train_games)\n",
    "        print('Train accuracy (policy):', train_acc_p)\n",
    "        print('Train loss (policy):', train_loss_p)\n",
    "        print('Train accuracy (rollout):', train_acc_r)\n",
    "        print('Train loss (rollout):', train_loss_r)\n",
    "        print('Valid acc (policy):', last_val_acc_p)\n",
    "        print('Valid loss (policy):', last_val_loss_p)\n",
    "        print('Valid acc (rollout):', last_val_acc_r)\n",
    "        print('Valid loss (rollout):', last_val_loss_r)\n",
    "        print('----------------------------------------------------')\n",
    "        return log_count\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_ind = 0\n",
    "        epoch_time = time.time()\n",
    "        for batch_X, batch_Y, batch_time, g_epoch, game_ind in train_gen:\n",
    "            info = sess.run([accuracy_p, loss_p, train_step_p, accuracy_r, loss_r, train_step_r], \n",
    "                            feed_dict={x: batch_X, y_: batch_Y, phase: True, alpha: alph})\n",
    "            train_acc_p = info[0]\n",
    "            train_loss_p = info[1]\n",
    "            train_acc_r = info[3]\n",
    "            train_loss_r = info[4]\n",
    "        \n",
    "            batch_ind += 1\n",
    "            log_count = info_log(log_count)\n",
    "            if batch_ind == batches_per_epoch:\n",
    "                break\n",
    "        valid_gen = TrainGen(batch_size, games_num-validation_games, aug=False, end=True)\n",
    "        valid_ind = 0\n",
    "        last_val_loss_p = 0\n",
    "        last_val_acc_p = 0\n",
    "        last_val_loss_r = 0\n",
    "        last_val_acc_r = 0\n",
    "        for valid_X, valid_Y, batch_time, g_epoch, game_ind in valid_gen:\n",
    "            info = sess.run([accuracy_p, loss_p, accuracy_r, loss_r], feed_dict={x: valid_X, y_: valid_Y, phase: False})\n",
    "            last_val_acc_p += info[0]\n",
    "            last_val_loss_p += info[1]\n",
    "            last_val_acc_r += info[2]\n",
    "            last_val_loss_r += info[3]\n",
    "            valid_ind += 1\n",
    "        last_val_acc_p /= valid_ind\n",
    "        last_val_loss_p /= valid_ind\n",
    "        last_val_acc_r /= valid_ind\n",
    "        last_val_loss_r /= valid_ind\n",
    "        saver.save(sess, name, global_step=epoch+1, write_meta_graph=False)\n",
    "        log_count = info_log(log_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UMznv-HlewFy"
   },
   "outputs": [],
   "source": [
    "graph_dict = AllGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19945,
     "status": "ok",
     "timestamp": 1527524280564,
     "user": {
      "displayName": "Роман Соколов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114170197370675117519"
     },
     "user_tz": -180
    },
    "id": "0-ukYcXWewF3",
    "outputId": "54e1af44-c1d2-463c-f0d0-92a645bf434f"
   },
   "outputs": [],
   "source": [
    "local_epochs = 35\n",
    "batches_per_epoch = 8000\n",
    "validation_games = 3000\n",
    "batch_size = 100\n",
    "alph = 0.0001\n",
    "name = 'checkpoints/allgraph'\n",
    "with tf.Session(graph=graph_dict['graph']) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        SupervisedTrain(sess, graph_dict, batches_per_epoch=batches_per_epoch, \n",
    "                                          validation_games=validation_games,\n",
    "                                          batch_size=batch_size,\n",
    "                                          alph=alph,\n",
    "                                          name=name,\n",
    "                                          local_epochs=epochs)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, name+'-fin', write_meta_graph=False)\n",
    "    except KeyboardInterrupt:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, name+'-fin', write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "SupervisedTrain.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
