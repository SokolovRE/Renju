{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 - Multilayer perceptron\n",
    "\n",
    "### Papers\n",
    "1. [TensorFlow](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf)\n",
    "\n",
    "### TensorFlow\n",
    "1. [Installing TensorFlow](https://www.tensorflow.org/install/)\n",
    "2. [Basics of TensorFlow](https://www.tensorflow.org/get_started/get_started)\n",
    "3. [Mnist with TensorFlow](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "4. [TensorFlow Mechanics](https://www.tensorflow.org/get_started/mnist/mechanics)\n",
    "5. [Visualization](https://www.tensorflow.org/get_started/graph_viz)\n",
    "\n",
    "\n",
    "### One more thing\n",
    "1. [Jupyter tutorial](https://habrahabr.ru/company/wunderfund/blog/316826/)\n",
    "2. [Plot.ly](https://plot.ly/python/)\n",
    "3. [Widgets](http://jupyter.org/widgets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear multi-classification problem\n",
    "\n",
    "We have already learned binary linear classifier\n",
    "$$y = \\text{sign}(w^Tx).$$\n",
    "There are [several approaches](https://en.wikipedia.org/wiki/Multiclass_classification) to solve the problem of multi-class classification. For example [reduction](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_Binary) of problem to binary classifier or [modification](https://en.wikipedia.org/wiki/Support_vector_machine#Multiclass_SVM) of the known model. However we are interested in approaches that is applied in neural networks.\n",
    "\n",
    "For each class $c \\in 1, \\dots, |C|$ we have an individual row $w_i$ of matrix $W$. Then the probability of $x$ belonging to a particular class is equal to\n",
    "$$p_i = \\frac{\\exp(w^T_ix)}{\\sum_j \\exp(w^T_jx)}.$$\n",
    "This is nothing, but [softmax](https://en.wikipedia.org/wiki/Softmax_function) function of $Wx$.\n",
    "$$(p_1, \\dots, p_{|C|}) = \\text{softmax}(Wx).$$\n",
    "\n",
    "If you look closely, $\\text{softmax}$ is a more general variant of sigmoid. To see this, it suffices to consider the case $|C|=2$. As usual the training can be reduced to minimization of the empirical risk, namely, optimization problem\n",
    "$$\\arg\\min_W Q(W) = \\arg\\min_W -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)).$$\n",
    "Actually, the maximization of the log-likelihood is written above.\n",
    "\n",
    "#### Exercises\n",
    "1. Find $\\frac{dQ}{dW}$ in matrix form (hint: start with $\\frac{dQ}{dw_i}$ for begining).\n",
    "2. Please plot several mnist images (e.g using grid 5x5).\n",
    "3. Train linear multi-label classifier for [mnist](https://www.kaggle.com/c/digit-recognizer) dataset with TensorFlow (possible, [this tutorial](https://www.tensorflow.org/get_started/mnist/pros) can help you).\n",
    "4. Chek accuracy on train and validation sets.\n",
    "5. Use a local [TensorBoard instance](https://www.tensorflow.org/get_started/graph_viz) to visualize resulted graph (no need to include in lab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы обозначим конкретное $i$ для которого $[y = i]$ то наша функция потерь будет выглядеть так:\n",
    "\n",
    "$$Q(W) = -\\frac{1}{\\mathcal{l}} \\ln(p_i(W)) = -\\frac{1}{\\mathcal{l}} \\ln{\\frac{\\exp(w^T_ix)}{\\sum\\limits_j \\exp(w^T_jx)}} = -\\frac{1}{\\mathcal{l}} \\big( \\ln{(\\exp(w^T_ix))} - \\ln{(\\sum\\limits_j \\exp(w^T_jx))} \\big) = -\\frac{1}{\\mathcal{l}} \\big( w^T_ix - \\ln{(\\sum\\limits_j \\exp(w^T_jx))} \\big)$$\n",
    "\n",
    "Возмём производную по $w_{ik}$:\n",
    "\n",
    "$$\\frac{dQ}{dw_{ik}} = -\\frac{1}{\\mathcal{l}} \\big( x_k - \\frac{\\exp(w^T_ix)}{\\sum\\limits_j \\exp(w^T_jx)} \\cdot x_k \\big) = -\\frac{1}{\\mathcal{l}} \\big( x_k - p_i \\cdot x_k \\big)$$\n",
    "\n",
    "Тогда по сути мы нашли $\\frac{dQ}{dw_i} = -\\frac{1}{\\mathcal{l}} (x^T - p_ix^T)$ где умножение на $p_i$ поэлементное. Для остальных строчек производная будет выглядеть так же только без $x^T$ в начале и берём $j$ элемент софтмакса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"train.csv\", sep = ',')\n",
    "mnist = mnist.sample(frac=1).values\n",
    "\n",
    "def parse_data(l, r, data):\n",
    "    X = data[l:r, 1:]\n",
    "    temp = []\n",
    "    Y = np.array([data[l:r,0]]).T\n",
    "    for i in range(len(Y)):\n",
    "        temp.append(np.zeros(10))\n",
    "        temp[i][Y[i]] += 1\n",
    "    Y = np.array(temp)\n",
    "    return X, Y\n",
    "\n",
    "train_X, train_Y = parse_data(0, int(len(mnist) * 0.7), mnist)\n",
    "valid_X, valid_Y = parse_data(int(len(mnist) * 0.7), int(len(mnist) * 0.85), mnist)\n",
    "test_X, test_Y = parse_data(int(len(mnist) * 0.85), len(mnist), mnist)\n",
    "\n",
    "len_X = len(train_X[0])\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAAOICAYAAAC+AC5hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJO2Ui6ckSZIkaaf+A77ehB7fIPaBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d9afe9be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        image = plt.subplot2grid((5, 5), (i, j))\n",
    "        image.imshow(np.resize(train_X[5*i + j], (28, 28)), cmap='gray_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "def gradient_descent(loss_function, sess, alpha=1e-4, epochs=10, batch_size=100, info=False):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with tf.name_scope('Training_step'):\n",
    "        train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n",
    "        \n",
    "    batch_separators = tuple(np.arange(batch_size, len(train_X), batch_size))\n",
    "    batch_X = np.vsplit(train_X, batch_separators)\n",
    "    batch_Y = np.vsplit(train_Y, batch_separators)\n",
    "    \n",
    "    epochs_left = epochs\n",
    "    while epochs_left:\n",
    "        epochs_left -= 1\n",
    "        for i in range(len(batch_X)):\n",
    "            sess.run(train_step, feed_dict={x: batch_X[i], y_: batch_Y[i]})\n",
    "        if info:\n",
    "            print('Эпоха номер:', epochs - epochs_left)\n",
    "            print('Точность на обучении:', sess.run(Accuracy, feed_dict={x: batch_X[0], y_: batch_Y[0]}))\n",
    "            print('Точность на валидации:', sess.run(Accuracy, feed_dict={x: valid_X, y_: valid_Y}))\n",
    "            \n",
    "    print(\"Качество модели на обучении:\", sess.run(Accuracy, feed_dict={x: batch_X[0], y_: batch_Y[0]}))\n",
    "    print(\"Качество модели на валидации:\", sess.run(Accuracy, feed_dict={x: valid_X, y_: valid_Y}))\n",
    "    print(\"Качество модели на тестировании:\", sess.run(Accuracy, feed_dict={x: test_X, y_: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Weights/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n\nCaused by op 'Training_step/beta1_power/read', defined at:\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b22ba2919f42>\", line 21, in <module>\n    gradient_descent(Loss_function, sess, epochs=10, alpha=0.01, batch_size=200)\n  File \"<ipython-input-4-44727462aca0>\", line 7, in gradient_descent\n    train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n    name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 126, in _create_slots\n    trainable=False)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1679, in variable\n    caching_device=caching_device, name=name, dtype=dtype)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Weights/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Weights/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b22ba2919f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-44727462aca0>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(loss_function, sess, alpha, epochs, batch_size, info)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepochs_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Эпоха номер:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepochs_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Weights/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n\nCaused by op 'Training_step/beta1_power/read', defined at:\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b22ba2919f42>\", line 21, in <module>\n    gradient_descent(Loss_function, sess, epochs=10, alpha=0.01, batch_size=200)\n  File \"<ipython-input-4-44727462aca0>\", line 7, in gradient_descent\n    train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n    name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 126, in _create_slots\n    trainable=False)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1679, in variable\n    caching_device=caching_device, name=name, dtype=dtype)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Weights/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, len_X], name='Inputs')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classes], name='Answers')\n",
    "\n",
    "    with tf.name_scope('Weights'):\n",
    "        W = get_param([len_X, classes], 'Weights')\n",
    "        b = get_param([classes], 'Bias')\n",
    "\n",
    "    with tf.name_scope('Layer'):\n",
    "        Layer = tf.matmul(x, W) + b\n",
    "\n",
    "    with tf.name_scope('Model_evaluation'):\n",
    "        Check_prediction = tf.equal(tf.argmax(Layer,1), tf.argmax(y_,1), name='Check_prediction')\n",
    "        Accuracy = tf.reduce_mean(tf.cast(Check_prediction, tf.float32), name='Accuracy')\n",
    "        \n",
    "    with tf.name_scope('Loss_function'):\n",
    "        Loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Layer), name='Loss')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    gradient_descent(Loss_function, sess, epochs=10, alpha=0.01, batch_size=200)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"logs/linear\")\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly touch on themes of regularization. As was discussed before, there are different approaches. We focus on the modification of loss function.\n",
    "\n",
    "$$\\arg\\min_W -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)) + \\lambda_1 L_1(W) + \\lambda_2 L_2(W)$$\n",
    "\n",
    "1. $L_1(W) = sum_{i,j} |w_{i,j}|$ - sparsify weights (force to not use uncorrelated features)\n",
    "2. $L_2(W) = sum_{i,j} w_{i,j}^2$ - minimize weights (force to not overfit)\n",
    "\n",
    "#### Exercises\n",
    "1. Train model again using both type of regularization.\n",
    "2. Plot matrix of weights.\n",
    "3. Which pixels have zero weights? What does it mean?\n",
    "4. Have you improved accuracy on validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_1 = 0.0005\n",
    "lamb_2 = 0.0005\n",
    "\n",
    "with graph.as_default():\n",
    "    L1 = tf.reduce_sum(tf.abs(W), name='L1_Regularization')\n",
    "    L2 = tf.reduce_sum(tf.multiply(W, W), name='L2_Regularization')\n",
    "\n",
    "    with tf.name_scope('Loss_function_with_regularization'):\n",
    "        Regularization_loss = Loss_function + lamb_1 * L1 + lamb_2 * L2\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:        \n",
    "    gradient_descent(Loss_function, sess, epochs=30, alpha=0.01, batch_size=200)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"logs/linear_with_regularization\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    print(sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется что матрица весов пустая и заполнена только строка отвечающая за bias однако это не так, дело в том что этот вывод показывает нам только края матрицы весов, а края матрицы весов отвечают за край картинки где пиксели почти всегда белые и не представляют ценности для классификации. Точность на валидации удалось улучшить не сильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Universal approximation theorem\n",
    "\n",
    "What if we add more layers to our model? Namely, we train two matrix $W_2$ and $W_1$\n",
    "$$softmax(W_2\\cdot(W_1x)).$$\n",
    "\n",
    "At first glance adding more parameters helps to increase the generalizing ability of the model. Buy actually we have the same model $softmax(Wx)$, where $W = W_2\\cdot W_1$. But everyting changes with adding ome more layer. Let's add nonlinear function $\\sigma$ between $W_2$ and $W_1$\n",
    "\n",
    "$$softmax(W_2\\cdot \\sigma(W_1x)).$$\n",
    "\n",
    "Kurt Hornik showed in 1991 that it is not the specific choice of the nonlinear function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case.\n",
    "\n",
    "Let $\\sigma(\\cdot)$ be a nonconstant, bounded, and monotonically-increasing continuous function.\n",
    "Let $\\mathcal{S}_m \\subset \\mathbb{R}^m$ denote any compact set. \n",
    "Then, given any $\\varepsilon > 0$ and any coninuous function $f$ on $\\mathcal{S}_m$, there exist an integer $N$ and real constants $v_i$, $b_i$ amd real vectors $w_i$ that\n",
    "\n",
    "$$\\left| \\sum _{i=1}^{N}v_{i}\\sigma \\left( w_i^{T} x+b_i \\right) - f(x) \\right| < \\varepsilon, ~~~ \\forall x \\in \\mathcal{S}_m.$$\n",
    "\n",
    "The theorem has non-constructive proof, it meams that no estimates for $N$ and no method to find approximation's parameters.\n",
    "\n",
    "#### Exercises\n",
    "1. Let $\\sigma$ – [heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) and $x \\in \\{0, 1\\}^2$. Prove that $y = \\sigma(wx + b)$ can approximate boolean function **OR** (hint: use constructive proof).\n",
    "2. What about **AND** function?\n",
    "3. Is it possible to implement **XOR**? Prove your words.\n",
    "4. Prove that 2-layer network can implement any boolean function.\n",
    "\n",
    "#### More useful facts:\n",
    "1. A 2-layer network in in $\\mathbb{R}^n$ allows to define convex polyhedron..\n",
    "2. A 3-layer network in в $\\mathbb{R}^n$ allows to define a not necessarily convex and not even necessarily connected area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation\n",
    "Backpropagation is a method used to calculate the error contribution of each layer after a batch of data. It is a special case of an older and more general technique called automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of layers by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers. The main motivation of method is simplify evaluation of gradient which is complex problem for multilayer nets.\n",
    "\n",
    "We need the following notation. Let $(y^1,\\dots,y^n) = f(x^1,\\dots,x^n)$ is some differentiable function and $\\frac{dy}{dx}$ is matrix\n",
    "$$\\frac{dy}{dx} = \\Big[ J_{ij} = \\frac{\\partial y^i}{\\partial x^j} \\Big]$$\n",
    "\n",
    "Without violating the generality, we can assume that each layer is a function $x_{i} = f(x_{i-1}, w_i)$. As last layer we add loss function, so we can assume our multi-layer net as function $Q(x_0) = Q(f_n(f_{n-1}(\\dots, w_{n-1}), w_n))$.\n",
    "\n",
    "#### Forward step\n",
    "Propagation forward through the network to generate the output values. Calculation of the loss function.\n",
    "\n",
    "#### Backward step\n",
    "Let's look at last layer. We can simply find $\\frac{dQ}{dx_n}$. Now we can evaluate \n",
    "\n",
    "$$\\frac{dQ}{dw_n} = \\frac{dQ}{dx_n}\\frac{dx_n}{dw_n} \\text{ and } \\frac{dQ}{dx_{n-1}} = \\frac{dQ}{dx_n}\\frac{dx_n}{dx_{n-1}}$$\n",
    "\n",
    "Now we need calculate $\\frac{dQ}{dw_{n-2}}$ и $\\frac{dQ}{dx_{n-2}}$. But we have the same situation. We know $\\frac{dQ}{dx_k}$, so can evaluate $\\frac{dQ}{dw_k}$ and $\\frac{dQ}{dx_{k-1}}$. Repeating this operation we find all the gradients. Now it's only remains to make a gradient step to update weights.\n",
    "\n",
    "#### Exercises\n",
    "1. Read more about [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "2. Train 2 layer net. Use sigmoid as nonlinearity.\n",
    "3. Check accuracy on validation set.\n",
    "4. Use [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) or LeakyReLu as nonlinearity. Compare accuracy and convergence with previous model.\n",
    "5. Play with different architectures (add more layers, regularization and etc).\n",
    "6. Show your best model.\n",
    "7. How does quality change with adding layers. Prove your words, train model for 2, 3, 5, 7 and 10 layers.\n",
    "8. Using backpropagation find optimal  digit 8 for your net.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, len_X], name='Inputs')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classes], name='Answers')\n",
    "    \n",
    "    with tf.name_scope('Weights_1'):\n",
    "        W_1 = get_param([len_X, neurons], 'Weights')\n",
    "        b_1 = get_param([neurons], 'Bias')\n",
    "        \n",
    "    with tf.name_scope('Weights_2'):\n",
    "        W_2 = get_param((neurons, classes), 'Weights')\n",
    "        b_2 = get_param([classes], 'Bias')\n",
    "\n",
    "    with tf.name_scope('First_layer'):\n",
    "        First_mul = tf.matmul(x, W_1, name='First_mul') + b_1\n",
    "        \n",
    "    Sigmoid = tf.sigmoid(First_mul, name='Sigmoid')\n",
    "        \n",
    "    with tf.name_scope('Second_layer'):\n",
    "        Second_mul = tf.matmul(Sigmoid, W_2, name='Second_mul') + b_2\n",
    "\n",
    "    with tf.name_scope('Model_evaluation'):\n",
    "        Check_prediction = tf.equal(tf.argmax(Second_mul,1), tf.argmax(y_,1), name='Check_prediction')\n",
    "        Accuracy = tf.reduce_mean(tf.cast(Check_prediction, tf.float32), name='Accuracy')\n",
    "\n",
    "    with tf.name_scope('Loss_function'):\n",
    "        Loss_function = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Second_mul), name='Loss')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    gradient_descent(Loss_function, sess, epochs=100, alpha=0.05, batch_size=450)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"logs/2_layer_net\")\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, len_X], name='Inputs')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classes], name='Answers')\n",
    "    \n",
    "    with tf.name_scope('Weights_1'):\n",
    "        W_1 = get_param([len_X, neurons], 'Weights')\n",
    "        b_1 = get_param([neurons], 'Bias')\n",
    "        \n",
    "    with tf.name_scope('Weights_2'):\n",
    "        W_2 = get_param((neurons, classes), 'Weights')\n",
    "        b_2 = get_param([classes], 'Bias')\n",
    "\n",
    "    with tf.name_scope('First_layer'):\n",
    "        First_mul = tf.matmul(x, W_1, name='First_mul') + b_1\n",
    "        \n",
    "    ReLu = tf.nn.relu(First_mul, name='ReLu')\n",
    "        \n",
    "    with tf.name_scope('Second_layer'):\n",
    "        Second_mul = tf.matmul(ReLu, W_2, name='Second_mul') + b_2\n",
    "\n",
    "    with tf.name_scope('Model_evaluation'):\n",
    "        Check_prediction = tf.equal(tf.argmax(Second_mul,1), tf.argmax(y_,1), name='Check_prediction')\n",
    "        Accuracy = tf.reduce_mean(tf.cast(Check_prediction, tf.float32), name='Accuracy')\n",
    "\n",
    "    with tf.name_scope('Loss_function'):\n",
    "        Loss_function = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Second_mul), name='Loss')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    gradient_descent(Loss_function, sess, epochs=100, alpha=0.05, batch_size=450, info=True)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"logs/2_layers_with_ReLu\")\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чёт качеству нехорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Conv_1/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n\nCaused by op 'Training_step/beta1_power/read', defined at:\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-b1f311cdfd1e>\", line 56, in <module>\n    gradient_descent(Loss_function, sess, epochs=20, batch_size=350, info=True)\n  File \"<ipython-input-4-44727462aca0>\", line 7, in gradient_descent\n    train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n    name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 126, in _create_slots\n    trainable=False)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1679, in variable\n    caching_device=caching_device, name=name, dtype=dtype)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Conv_1/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Conv_1/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b1f311cdfd1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/Tensorflow_example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-44727462aca0>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(loss_function, sess, alpha, epochs, batch_size, info)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepochs_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Эпоха номер:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepochs_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Conv_1/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n\nCaused by op 'Training_step/beta1_power/read', defined at:\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/roman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-b1f311cdfd1e>\", line 56, in <module>\n    gradient_descent(Loss_function, sess, epochs=20, batch_size=350, info=True)\n  File \"<ipython-input-4-44727462aca0>\", line 7, in gradient_descent\n    train_step = tf.train.AdamOptimizer(alpha).minimize(loss_function)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n    name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 126, in _create_slots\n    trainable=False)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1679, in variable\n    caching_device=caching_device, name=name, dtype=dtype)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/roman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Training_step/beta1_power\n\t [[Node: Training_step/beta1_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@Conv_1/Bias\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Training_step/beta1_power)]]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, len_X], name='Inputs')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, classes], name='Answers')\n",
    "    \n",
    "    Images = tf.reshape(x, [-1, 28, 28, 1], name='Reshape')\n",
    "    \n",
    "    with tf.name_scope('Conv_1'):\n",
    "        W_1 = get_param([5, 5, 1, 32], 'Weights')\n",
    "        b_1 = get_param([32], 'Bias')\n",
    "        \n",
    "        Conv_1 = tf.nn.conv2d(Images, W_1, strides=[1, 1, 1, 1], padding='SAME', name='Conv') + b_1\n",
    "        ReLu_1 = tf.sigmoid(Conv_1, name='ReLu')\n",
    "        \n",
    "    Max_pool_1 = tf.nn.max_pool(ReLu_1, ksize=[1, 2, 2, 1], \n",
    "                                strides=[1, 2, 2, 1], padding='SAME', name='Max_pool_1')\n",
    "    \n",
    "    with tf.name_scope('Conv_2'):\n",
    "        W_2 = get_param([5, 5, 32, 64], 'Weights')\n",
    "        b_2 = get_param([64], 'Bias')\n",
    "        \n",
    "        Conv_2 = tf.nn.conv2d(Max_pool_1, W_2, strides=[1, 1, 1, 1], padding='SAME', name='Conv') + b_2\n",
    "        ReLu_2 = tf.sigmoid(Conv_2, name='ReLu')\n",
    "        \n",
    "    Max_pool_2 = tf.nn.max_pool(ReLu_2, ksize=[1, 2, 2, 1], \n",
    "                                strides=[1, 2, 2, 1], padding='SAME', name='Max_pool_2')\n",
    "    \n",
    "    Vectors = tf.reshape(Max_pool_2, [-1, 7*7*64], name='Reshape')\n",
    "    \n",
    "    with tf.name_scope('Dance'):\n",
    "        W_3 = get_param([7*7*64, 1024], 'Weights')\n",
    "        b_3 = get_param([1024], 'Bias')\n",
    "        \n",
    "        Layer = tf.matmul(Vectors, W_3, name='Matmul') + b_3\n",
    "        ReLu_3 = tf.sigmoid(Layer, name='ReLu')\n",
    "        '''\n",
    "    with tf.name_scope('Dropout'):\n",
    "        Prob = tf.placeholder(tf.float32, name='Prob')\n",
    "        Dropout = tf.nn.dropout(ReLu_3, Prob)\n",
    "        '''\n",
    "    with tf.name_scope('Read_out'):\n",
    "        W_4 = get_param([1024, 10], 'Weights')\n",
    "        b_4 = get_param([10], 'Bias')\n",
    "        \n",
    "        Read_out = tf.matmul(ReLu_3, W_4, name='Read_out') + b_4\n",
    "\n",
    "    with tf.name_scope('Model_evaluation'):\n",
    "        Check_prediction = tf.equal(tf.argmax(Read_out,1), tf.argmax(y_,1), name='Check_prediction')\n",
    "        Accuracy = tf.reduce_mean(tf.cast(Check_prediction, tf.float32), name='Accuracy')\n",
    "\n",
    "    with tf.name_scope('Loss_function'):\n",
    "        Loss_function = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Read_out), name='Loss')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    gradient_descent(Loss_function, sess, epochs=20, batch_size=350, info=True)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"logs/Tensorflow_example\")\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Autoencoders\n",
    "An autoencoder is an network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Also, this technique can be used to train deep nets.\n",
    "\n",
    "Architecturally, the simplest form of an autoencoder is a feedforward net very similar to the multilayer perceptron (MLP), but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs. Therefore, autoencoders are unsupervised learning models. An autoencoder always consists of two parts, the encoder and the decoder. Encoder returns latent representation of the object (compressed representation, usuallu smaller dimension), but decoder restores object from this latent representation. Autoencoders are also trained to minimise reconstruction errors (e.g. MSE).\n",
    "\n",
    "Various techniques exist to prevent autoencoders from learning the identity and to improve their ability to capture important information:\n",
    "1. Denoising autoencoder - take a partially corrupted input.\n",
    "2. Sparse autoencoder - impose sparsity on the hidden units during training (whilst having a larger number of hidden units than inputs).\n",
    "3. Variational autoencoder models inherit autoencoder architecture, but make strong assumptions concerning the distribution of latent variables.\n",
    "4. Contractive autoencoder - add an explicit regularizer in objective function that forces the model to learn a function that is robust to slight variations of input values.\n",
    "\n",
    "#### Exercises\n",
    "1. Train 2 layers autoencoder that compressed mnist images to $\\mathbb{R}^3$ space.\n",
    "2. For each digit plot several samples in 3D axis (use \"%matplotlib notebook\" mode or plotly). How do digits group?\n",
    "3. Train autoencoder with more layers. What are results?\n",
    "4. Use autoencoder to pretrain 2 layers (unsupervised) and then train the following layers with supervised method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
